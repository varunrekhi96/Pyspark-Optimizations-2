{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3fb688b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "orders_schema = order_id int, order_date string, customer_id int,order_status string\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "order_id int, order_date string, customer_id int,order_status string"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val orders_schema = \"order_id int, order_date string, customer_id int,order_status string\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07aef422",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Waiting for a Spark session to start..."
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "orders_df = [order_id: int, order_date: string ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[order_id: int, order_date: string ... 2 more fields]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val orders_df = spark.read.format(\"csv\").schema(orders_schema).load(\"/public/trendytech/orders/orders_1gb.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "301cdb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df.createOrReplaceTempView(\"orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9be54f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------------+\n",
      "|new_order_id|   order_status|\n",
      "+------------+---------------+\n",
      "|           1|         CLOSED|\n",
      "|           2|PENDING_PAYMENT|\n",
      "|           3|       COMPLETE|\n",
      "|           4|         CLOSED|\n",
      "|           5|       COMPLETE|\n",
      "|           6|       COMPLETE|\n",
      "|           7|       COMPLETE|\n",
      "|           8|     PROCESSING|\n",
      "|           9|PENDING_PAYMENT|\n",
      "|          10|PENDING_PAYMENT|\n",
      "|          11| PAYMENT_REVIEW|\n",
      "|          12|         CLOSED|\n",
      "|          13|PENDING_PAYMENT|\n",
      "|          14|     PROCESSING|\n",
      "|          15|       COMPLETE|\n",
      "|          16|PENDING_PAYMENT|\n",
      "|          17|       COMPLETE|\n",
      "|          18|         CLOSED|\n",
      "|          19|PENDING_PAYMENT|\n",
      "|          20|     PROCESSING|\n",
      "+------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select order_id*1 as new_order_id, order_status from\n",
    "(select order_id, customer_id, order_status from orders where order_id < 500)\n",
    "where order_id < 200\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b30c2017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Project [('order_id * 1) AS new_order_id#20, 'order_status]\n",
      "+- 'Filter ('order_id < 200)\n",
      "   +- 'SubqueryAlias `__auto_generated_subquery_name`\n",
      "      +- 'Project ['order_id, 'customer_id, 'order_status]\n",
      "         +- 'Filter ('order_id < 500)\n",
      "            +- 'UnresolvedRelation `orders`\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "new_order_id: int, order_status: string\n",
      "Project [(order_id#0 * 1) AS new_order_id#20, order_status#3]\n",
      "+- Filter (order_id#0 < 200)\n",
      "   +- SubqueryAlias `__auto_generated_subquery_name`\n",
      "      +- Project [order_id#0, customer_id#2, order_status#3]\n",
      "         +- Filter (order_id#0 < 500)\n",
      "            +- SubqueryAlias `orders`\n",
      "               +- Relation[order_id#0,order_date#1,customer_id#2,order_status#3] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [(order_id#0 * 1) AS new_order_id#20, order_status#3]\n",
      "+- Filter ((isnotnull(order_id#0) && (order_id#0 < 500)) && (order_id#0 < 200))\n",
      "   +- Relation[order_id#0,order_date#1,customer_id#2,order_status#3] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Project [(order_id#0 * 1) AS new_order_id#20, order_status#3]\n",
      "+- *(1) Filter ((isnotnull(order_id#0) && (order_id#0 < 500)) && (order_id#0 < 200))\n",
      "   +- *(1) FileScan csv [order_id#0,order_status#3] Batched: false, Format: CSV, Location: InMemoryFileIndex[hdfs://m01.itversity.com:9000/public/trendytech/orders/orders_1gb.csv], PartitionFilters: [], PushedFilters: [IsNotNull(order_id), LessThan(order_id,500), LessThan(order_id,200)], ReadSchema: struct<order_id:int,order_status:string>\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select order_id*1 as new_order_id, order_status from\n",
    "(select order_id, customer_id, order_status from orders where order_id < 500)\n",
    "where order_id < 200\"\"\").explain(true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3a0478a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.catalyst.expressions.Literal\n",
    "import org.apache.spark.sql.catalyst.rules.Rule\n",
    "import org.apache.spark.sql.catalyst.plans.logical.LogicalPlan\n",
    "import org.apache.spark.sql.catalyst.expressions._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92fbcd59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined object MultiplyOptimizationRule\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    }
   ],
   "source": [
    "object MultiplyOptimizationRule extends Rule[LogicalPlan] {\n",
    "def apply(plan: LogicalPlan): LogicalPlan = plan transformAllExpressions {\n",
    "case Multiply(left,right) if right.isInstanceOf[Literal] &&\n",
    "right.asInstanceOf[Literal].value.asInstanceOf[Integer] == 1 =>\n",
    "println(\"optimization of one applied\")\n",
    "left\n",
    "}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17d858af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark.experimental.extraOptimizations: Seq[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]] = List(MultiplyOptimizationRule$@103a5ac6)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    }
   ],
   "source": [
    "spark.experimental.extraOptimizations = Seq(MultiplyOptimizationRule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3ff03b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimization of one applied\n",
      "== Parsed Logical Plan ==\n",
      "'Project [('order_id * 1) AS new_order_id#24, 'order_status]\n",
      "+- 'Filter ('order_id < 200)\n",
      "   +- 'SubqueryAlias `__auto_generated_subquery_name`\n",
      "      +- 'Project ['order_id, 'customer_id, 'order_status]\n",
      "         +- 'Filter ('order_id < 500)\n",
      "            +- 'UnresolvedRelation `orders`\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "new_order_id: int, order_status: string\n",
      "Project [(order_id#0 * 1) AS new_order_id#24, order_status#3]\n",
      "+- Filter (order_id#0 < 200)\n",
      "   +- SubqueryAlias `__auto_generated_subquery_name`\n",
      "      +- Project [order_id#0, customer_id#2, order_status#3]\n",
      "         +- Filter (order_id#0 < 500)\n",
      "            +- SubqueryAlias `orders`\n",
      "               +- Relation[order_id#0,order_date#1,customer_id#2,order_status#3] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [order_id#0 AS new_order_id#24, order_status#3]\n",
      "+- Filter ((isnotnull(order_id#0) && (order_id#0 < 500)) && (order_id#0 < 200))\n",
      "   +- Relation[order_id#0,order_date#1,customer_id#2,order_status#3] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Project [order_id#0 AS new_order_id#24, order_status#3]\n",
      "+- *(1) Filter ((isnotnull(order_id#0) && (order_id#0 < 500)) && (order_id#0 < 200))\n",
      "   +- *(1) FileScan csv [order_id#0,order_status#3] Batched: false, Format: CSV, Location: InMemoryFileIndex[hdfs://m01.itversity.com:9000/public/trendytech/orders/orders_1gb.csv], PartitionFilters: [], PushedFilters: [IsNotNull(order_id), LessThan(order_id,500), LessThan(order_id,200)], ReadSchema: struct<order_id:int,order_status:string>\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select order_id*1 as new_order_id, order_status from\n",
    "(select order_id, customer_id, order_status from orders where order_id < 500)\n",
    "where order_id < 200\"\"\").explain(true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fb6644",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark 2 - Scala",
   "language": "scala",
   "name": "spark_2_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
