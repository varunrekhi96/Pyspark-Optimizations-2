{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ffec928",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import getpass\n",
    "username = getpass.getuser()\n",
    "spark = SparkSession. \\\n",
    "builder. \\\n",
    "config('spark.ui.port', '0'). \\\n",
    "config('spark.shuffle.useOldFetchProtocol', 'true'). \\\n",
    "config(\"spark.sql.warehouse.dir\", f\"/user/{username}/warehouse\"). \\\n",
    "enableHiveSupport(). \\\n",
    "master('yarn'). \\\n",
    "getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d47a3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_schema = \"order_id long , order_date string, customer_id long,order_status string\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9429c788",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df = spark.read \\\n",
    ".format(\"csv\") \\\n",
    ".schema(orders_schema) \\\n",
    ".load(\"/public/trendytech/orders/orders_1gb.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "945519d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------+---------------+\n",
      "|order_id|          order_date|customer_id|   order_status|\n",
      "+--------+--------------------+-----------+---------------+\n",
      "|       1|2013-07-25 00:00:...|      11599|         CLOSED|\n",
      "|       2|2013-07-25 00:00:...|        256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25 00:00:...|      12111|       COMPLETE|\n",
      "|       4|2013-07-25 00:00:...|       8827|         CLOSED|\n",
      "|       5|2013-07-25 00:00:...|      11318|       COMPLETE|\n",
      "|       6|2013-07-25 00:00:...|       7130|       COMPLETE|\n",
      "|       7|2013-07-25 00:00:...|       4530|       COMPLETE|\n",
      "|       8|2013-07-25 00:00:...|       2911|     PROCESSING|\n",
      "|       9|2013-07-25 00:00:...|       5657|PENDING_PAYMENT|\n",
      "|      10|2013-07-25 00:00:...|       5648|PENDING_PAYMENT|\n",
      "|      11|2013-07-25 00:00:...|        918| PAYMENT_REVIEW|\n",
      "|      12|2013-07-25 00:00:...|       1837|         CLOSED|\n",
      "|      13|2013-07-25 00:00:...|       9149|PENDING_PAYMENT|\n",
      "|      14|2013-07-25 00:00:...|       9842|     PROCESSING|\n",
      "|      15|2013-07-25 00:00:...|       2568|       COMPLETE|\n",
      "|      16|2013-07-25 00:00:...|       7276|PENDING_PAYMENT|\n",
      "|      17|2013-07-25 00:00:...|       2667|       COMPLETE|\n",
      "|      18|2013-07-25 00:00:...|       1205|         CLOSED|\n",
      "|      19|2013-07-25 00:00:...|       9488|PENDING_PAYMENT|\n",
      "|      20|2013-07-25 00:00:...|       9198|     PROCESSING|\n",
      "+--------+--------------------+-----------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f9ae1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df.createOrReplaceTempView(\"orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0a0dd82",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Table or view not found: order; line 1 pos 14;\n'Project [*]\n+- 'UnresolvedRelation [order], [], false\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-8d481b2c4964>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"select * from order\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m         \"\"\"\n\u001b[0;32m--> 723\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    724\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtableName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Table or view not found: order; line 1 pos 14;\n'Project [*]\n+- 'UnresolvedRelation [order], [], false\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from order\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c25ba173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------+\n",
      "|order_id|   order_status|\n",
      "+--------+---------------+\n",
      "|       1|         CLOSED|\n",
      "|       2|PENDING_PAYMENT|\n",
      "|       3|       COMPLETE|\n",
      "|       4|         CLOSED|\n",
      "|       5|       COMPLETE|\n",
      "|       6|       COMPLETE|\n",
      "|       7|       COMPLETE|\n",
      "|       8|     PROCESSING|\n",
      "|       9|PENDING_PAYMENT|\n",
      "|      10|PENDING_PAYMENT|\n",
      "|      11| PAYMENT_REVIEW|\n",
      "|      12|         CLOSED|\n",
      "|      13|PENDING_PAYMENT|\n",
      "|      14|     PROCESSING|\n",
      "|      15|       COMPLETE|\n",
      "|      16|PENDING_PAYMENT|\n",
      "|      17|       COMPLETE|\n",
      "|      18|         CLOSED|\n",
      "|      19|PENDING_PAYMENT|\n",
      "|      20|     PROCESSING|\n",
      "+--------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\" select order_id, order_status from\n",
    "(select order_id, customer_id, order_status from orders\n",
    "where order_id < 500) where order_id < 200\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08eedda1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Project ['order_id, 'order_status]\n",
      "+- 'Filter ('order_id < 200)\n",
      "   +- 'SubqueryAlias __auto_generated_subquery_name\n",
      "      +- 'Project ['order_id, 'customer_id, 'order_status]\n",
      "         +- 'Filter ('order_id < 500)\n",
      "            +- 'UnresolvedRelation [orders], [], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "order_id: bigint, order_status: string\n",
      "Project [order_id#0L, order_status#3]\n",
      "+- Filter (order_id#0L < cast(200 as bigint))\n",
      "   +- SubqueryAlias __auto_generated_subquery_name\n",
      "      +- Project [order_id#0L, customer_id#2L, order_status#3]\n",
      "         +- Filter (order_id#0L < cast(500 as bigint))\n",
      "            +- SubqueryAlias orders\n",
      "               +- Relation[order_id#0L,order_date#1,customer_id#2L,order_status#3] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [order_id#0L, order_status#3]\n",
      "+- Filter ((isnotnull(order_id#0L) AND (order_id#0L < 500)) AND (order_id#0L < 200))\n",
      "   +- Relation[order_id#0L,order_date#1,customer_id#2L,order_status#3] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Filter ((isnotnull(order_id#0L) AND (order_id#0L < 500)) AND (order_id#0L < 200))\n",
      "+- FileScan csv [order_id#0L,order_status#3] Batched: false, DataFilters: [isnotnull(order_id#0L), (order_id#0L < 500), (order_id#0L < 200)], Format: CSV, Location: InMemoryFileIndex[hdfs://m01.itversity.com:9000/public/trendytech/orders/orders_1gb.csv], PartitionFilters: [], PushedFilters: [IsNotNull(order_id), LessThan(order_id,500), LessThan(order_id,200)], ReadSchema: struct<order_id:bigint,order_status:string>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\" select order_id, order_status from\n",
    "(select order_id, customer_id, order_status from orders\n",
    "where order_id < 500) where order_id < 200\n",
    "\"\"\").explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b1aac8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_schema = \" customerid long , customer_fname string , customer_lname string , user_name string,password string , address string, city string, state string, pincode long \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d71ca09",
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_df = spark.read \\\n",
    ".format(\"csv\") \\\n",
    ".schema(customer_schema) \\\n",
    ".load(\"/public/trendytech/retail_db/customers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "715014fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_df.createOrReplaceTempView(\"customers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b9ebebaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------+------------+----------+--------------+--------------+---------+---------+--------------------+------------+-----+-------+\n",
      "|order_id|          order_date|customer_id|order_status|customerid|customer_fname|customer_lname|user_name| password|             address|        city|state|pincode|\n",
      "+--------+--------------------+-----------+------------+----------+--------------+--------------+---------+---------+--------------------+------------+-----+-------+\n",
      "|       1|2013-07-25 00:00:...|      11599|      CLOSED|     11599|          Mary|        Malone|XXXXXXXXX|XXXXXXXXX|8708 Indian Horse...|     Hickory|   NC|  28601|\n",
      "|       4|2013-07-25 00:00:...|       8827|      CLOSED|      8827|         Brian|        Wilson|XXXXXXXXX|XXXXXXXXX|   8396 High Corners| San Antonio|   TX|  78240|\n",
      "|      12|2013-07-25 00:00:...|       1837|      CLOSED|      1837|          Mary|          Vega|XXXXXXXXX|XXXXXXXXX|  4312 Bright Corner|      Caguas|   PR|    725|\n",
      "|      18|2013-07-25 00:00:...|       1205|      CLOSED|      1205|          Mary|        Powell|XXXXXXXXX|XXXXXXXXX|9299 Quiet Pionee...|       Miami|   FL|  33126|\n",
      "|      24|2013-07-25 00:00:...|      11441|      CLOSED|     11441|          Mary|      Ferguson|XXXXXXXXX|XXXXXXXXX|8702 Umber Mounta...|        Lutz|   FL|  33549|\n",
      "|      25|2013-07-25 00:00:...|       9503|      CLOSED|      9503|          Mary|   Fitzpatrick|XXXXXXXXX|XXXXXXXXX|    7861 Honey Acres|     Orlando|   FL|  32822|\n",
      "|      37|2013-07-25 00:00:...|       5863|      CLOSED|      5863|           Amy|         Smith|XXXXXXXXX|XXXXXXXXX|8187 Cotton Rise ...|     Cordova|   TN|  38018|\n",
      "|      51|2013-07-25 00:00:...|      12271|      CLOSED|     12271|          Mary|         Small|XXXXXXXXX|XXXXXXXXX|6599 Grand Island...|     Ontario|   CA|  91761|\n",
      "|      57|2013-07-25 00:00:...|       7073|      CLOSED|      7073|          Joan|         Smith|XXXXXXXXX|XXXXXXXXX|1987 Grand Conces...|     Del Rio|   TX|  78840|\n",
      "|      61|2013-07-25 00:00:...|       4791|      CLOSED|      4791|          Mary|        Patton|XXXXXXXXX|XXXXXXXXX|    4696 Shady Mount|     Hialeah|   FL|  33010|\n",
      "|      62|2013-07-25 00:00:...|       9111|      CLOSED|      9111|          Mary|         Smith|XXXXXXXXX|XXXXXXXXX|  2122 Green By-pass|      Caguas|   PR|    725|\n",
      "|      87|2013-07-25 00:00:...|       3065|      CLOSED|      3065|           Amy|      Cummings|XXXXXXXXX|XXXXXXXXX|  9658 Merry Landing|      Caguas|   PR|    725|\n",
      "|      90|2013-07-25 00:00:...|       9131|      CLOSED|      9131|          Mary|         Smith|XXXXXXXXX|XXXXXXXXX|3871 Umber Blosso...|  New Castle|   DE|  19720|\n",
      "|     101|2013-07-25 00:00:...|       5116|      CLOSED|      5116|         Doris|         Smith|XXXXXXXXX|XXXXXXXXX|     1819 Noble Ramp|    Brockton|   MA|   2301|\n",
      "|     116|2013-07-26 00:00:...|       8763|      CLOSED|      8763|        Pamela|      Williams|XXXXXXXXX|XXXXXXXXX|5567 Cotton Timbe...|      Santee|   CA|  92071|\n",
      "|     129|2013-07-26 00:00:...|       9937|      CLOSED|      9937|          Mary|        Atkins|XXXXXXXXX|XXXXXXXXX|8766 Easy Leaf Knoll| West Covina|   CA|  91790|\n",
      "|     133|2013-07-26 00:00:...|      10604|      CLOSED|     10604|      Jennifer|         Smith|XXXXXXXXX|XXXXXXXXX|   2905 Sleepy Route|Fayetteville|   NC|  28314|\n",
      "|     191|2013-07-26 00:00:...|         16|      CLOSED|        16|       Tiffany|         Smith|XXXXXXXXX|XXXXXXXXX|      6651 Iron Port|      Caguas|   PR|    725|\n",
      "|     201|2013-07-26 00:00:...|       9055|      CLOSED|      9055|         Karen|          Ross|XXXXXXXXX|XXXXXXXXX|5207 Dewy Butterf...|      Caguas|   PR|    725|\n",
      "|     211|2013-07-26 00:00:...|      10372|      CLOSED|     10372|         Amber|        Martin|XXXXXXXXX|XXXXXXXXX|  6481 Golden Island|      Caguas|   PR|    725|\n",
      "+--------+--------------------+-----------+------------+----------+--------------+--------------+---------+---------+--------------------+------------+-----+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select * from orders inner join customers \n",
    "on orders.customer_id == customers.customerid where \n",
    "order_status = 'CLOSED'\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d2d03131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Project [*]\n",
      "+- 'Filter ('order_status = CLOSED)\n",
      "   +- 'Join Inner, ('orders.customer_id = 'customers.customerid)\n",
      "      :- 'UnresolvedRelation [orders], [], false\n",
      "      +- 'UnresolvedRelation [customers], [], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "order_id: bigint, order_date: string, customer_id: bigint, order_status: string, customerid: bigint, customer_fname: string, customer_lname: string, user_name: string, password: string, address: string, city: string, state: string, pincode: bigint\n",
      "Project [order_id#0L, order_date#1, customer_id#2L, order_status#3, customerid#44L, customer_fname#45, customer_lname#46, user_name#47, password#48, address#49, city#50, state#51, pincode#52L]\n",
      "+- Filter (order_status#3 = CLOSED)\n",
      "   +- Join Inner, (customer_id#2L = customerid#44L)\n",
      "      :- SubqueryAlias orders\n",
      "      :  +- Relation[order_id#0L,order_date#1,customer_id#2L,order_status#3] csv\n",
      "      +- SubqueryAlias customers\n",
      "         +- Relation[customerid#44L,customer_fname#45,customer_lname#46,user_name#47,password#48,address#49,city#50,state#51,pincode#52L] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Join Inner, (customer_id#2L = customerid#44L)\n",
      ":- Filter ((isnotnull(order_status#3) AND (order_status#3 = CLOSED)) AND isnotnull(customer_id#2L))\n",
      ":  +- Relation[order_id#0L,order_date#1,customer_id#2L,order_status#3] csv\n",
      "+- Filter isnotnull(customerid#44L)\n",
      "   +- Relation[customerid#44L,customer_fname#45,customer_lname#46,user_name#47,password#48,address#49,city#50,state#51,pincode#52L] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "*(2) BroadcastHashJoin [customer_id#2L], [customerid#44L], Inner, BuildRight, false\n",
      ":- *(2) Filter ((isnotnull(order_status#3) AND (order_status#3 = CLOSED)) AND isnotnull(customer_id#2L))\n",
      ":  +- FileScan csv [order_id#0L,order_date#1,customer_id#2L,order_status#3] Batched: false, DataFilters: [isnotnull(order_status#3), (order_status#3 = CLOSED), isnotnull(customer_id#2L)], Format: CSV, Location: InMemoryFileIndex[hdfs://m01.itversity.com:9000/public/trendytech/orders/orders_1gb.csv], PartitionFilters: [], PushedFilters: [IsNotNull(order_status), EqualTo(order_status,CLOSED), IsNotNull(customer_id)], ReadSchema: struct<order_id:bigint,order_date:string,customer_id:bigint,order_status:string>\n",
      "+- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, false]),false), [id=#114]\n",
      "   +- *(1) Filter isnotnull(customerid#44L)\n",
      "      +- FileScan csv [customerid#44L,customer_fname#45,customer_lname#46,user_name#47,password#48,address#49,city#50,state#51,pincode#52L] Batched: false, DataFilters: [isnotnull(customerid#44L)], Format: CSV, Location: InMemoryFileIndex[hdfs://m01.itversity.com:9000/public/trendytech/retail_db/customers], PartitionFilters: [], PushedFilters: [IsNotNull(customerid)], ReadSchema: struct<customerid:bigint,customer_fname:string,customer_lname:string,user_name:string,password:st...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select * from orders inner join customers\n",
    "on orders.customer_id == customers.customerid where\n",
    "order_status = 'CLOSED'\"\"\").explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "75e5487e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+\n",
      "|customer_id|count(1)|\n",
      "+-----------+--------+\n",
      "|          1|     375|\n",
      "+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select customer_id, count(1) from\n",
    "(select * from orders where customer_id in (1,2,3,4,5))\n",
    "where customer_id in (1,2,3) group by customer_id\n",
    "having customer_id = 1\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4ea614",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"select customer_id, count(1) from\n",
    "(select * from orders where customer_id in (1,2,3,4,5))\n",
    "where customer_id in (1,2,3) group by customer_id\n",
    "having customer_id = 1\"\"\").explai"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 3",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
